# ğŸ¤– DetecciÃ³n AutomÃ¡tica de Modelos

## âœ… Nueva Funcionalidad Implementada

El sistema ahora **detecta automÃ¡ticamente** cuando un modelo de Groq no estÃ¡ disponible y prueba con otros modelos.

## ğŸ¯ CÃ³mo Funciona

### DetecciÃ³n AutomÃ¡tica en Tiempo Real

Cuando el chat intenta usar un modelo y recibe error:

```
âš ï¸ Error de Groq: The model 'mixtral-8x7b-32768' has been decommissioned
```

El sistema automÃ¡ticamente:
1. âœ… Detecta que el modelo no estÃ¡ disponible
2. âœ… Prueba con el siguiente modelo de la lista
3. âœ… ContinÃºa hasta encontrar uno que funcione
4. âœ… Si ninguno funciona, usa modo Fallback

### Lista de Modelos (Orden de Preferencia)

```python
1. llama-3.1-70b-versatile      # Recomendado (balance calidad/velocidad)
2. llama-3.2-90b-text-preview   # MÃ¡s potente
3. llama-3.1-8b-instant         # MÃ¡s rÃ¡pido
4. mixtral-8x7b-32768           # Legacy (por si vuelve)
```

## ğŸ” Detectar Modelos Disponibles Manualmente

### Ejecutar Script de DetecciÃ³n

```cmd
detect-models.bat
```

O manualmente:
```cmd
cd src\ai
python auto_detect_models.py
```

### Resultado Esperado

```
ğŸ” Detectando modelos disponibles en Groq...

Probando: llama-3.1-70b-versatile... âœ… Disponible
Probando: llama-3.2-90b-text-preview... âœ… Disponible
Probando: llama-3.1-8b-instant... âœ… Disponible
Probando: mixtral-8x7b-32768... âŒ Descontinuado

==================================================
âœ… Modelos disponibles: 3
  - llama-3.1-70b-versatile
  - llama-3.2-90b-text-preview
  - llama-3.1-8b-instant

âŒ Modelos descontinuados: 1
  - mixtral-8x7b-32768
==================================================

ğŸ’¡ RecomendaciÃ³n: Usa 'llama-3.1-70b-versatile' como modelo principal
```

## ğŸ”„ Flujo de Failover AutomÃ¡tico

```
Usuario: "Hola"
    â†“
Intenta con: llama-3.1-70b-versatile
    â†“
Â¿Funciona? â†’ SÃ­ â†’ âœ… Responde
    â†“
    No (modelo descontinuado)
    â†“
Intenta con: llama-3.2-90b-text-preview
    â†“
Â¿Funciona? â†’ SÃ­ â†’ âœ… Responde
    â†“
    No
    â†“
Intenta con: llama-3.1-8b-instant
    â†“
Â¿Funciona? â†’ SÃ­ â†’ âœ… Responde
    â†“
    No (todos fallaron)
    â†“
Modo Fallback (respuestas basadas en reglas)
```

## ğŸ“Š ComparaciÃ³n de Modelos

| Modelo | TamaÃ±o | Velocidad | Calidad | Estado |
|--------|--------|-----------|---------|--------|
| llama-3.1-70b-versatile | 70B | âš¡âš¡âš¡ | â­â­â­â­ | âœ… Activo |
| llama-3.2-90b-text-preview | 90B | âš¡âš¡ | â­â­â­â­â­ | âœ… Activo |
| llama-3.1-8b-instant | 8B | âš¡âš¡âš¡âš¡âš¡ | â­â­â­ | âœ… Activo |
| mixtral-8x7b-32768 | 47B | âš¡âš¡âš¡ | â­â­â­â­ | âŒ Descontinuado |

## ğŸ› ï¸ ConfiguraciÃ³n Manual (Opcional)

Si quieres forzar un modelo especÃ­fico, edita `src/ai/ai_assistant.py`:

```python
available_models = [
    "llama-3.2-90b-text-preview",  # Cambia el orden
    "llama-3.1-70b-versatile",
    "llama-3.1-8b-instant"
]
```

## ğŸ”§ Archivos Creados/Modificados

### Nuevos Archivos

1. **`detect-models.bat`** - Script para detectar modelos disponibles
2. **`src/ai/auto_detect_models.py`** - LÃ³gica de detecciÃ³n
3. **`AUTO_MODEL_DETECTION.md`** - Esta documentaciÃ³n

### Archivos Modificados

1. **`src/ai/ai_assistant.py`**:
   - Lista de modelos disponibles
   - DetecciÃ³n automÃ¡tica de modelos descontinuados
   - Failover automÃ¡tico entre modelos
   - Logs informativos

## âœ… Ventajas

1. **AutomÃ¡tico**: No requiere intervenciÃ³n manual
2. **Resiliente**: Si un modelo falla, prueba con otro
3. **Actualizable**: FÃ¡cil agregar nuevos modelos a la lista
4. **Informativo**: Logs claros de quÃ© modelo se estÃ¡ usando
5. **Fallback**: Siempre funciona, incluso sin modelos disponibles

## ğŸš€ Uso

### No Requiere ConfiguraciÃ³n

El sistema funciona automÃ¡ticamente. Solo:

1. **Reinicia el servidor de IA**:
```cmd
.\start-ai-server.bat
```

2. **Usa el chat normalmente**

El sistema detectarÃ¡ y usarÃ¡ el mejor modelo disponible.

### Verificar QuÃ© Modelo EstÃ¡ Usando

En los logs del servidor verÃ¡s:
```
ğŸ¤– Usando Groq con key: ...KmiTZ
âš ï¸ Modelo 'mixtral-8x7b-32768' no disponible, probando siguiente...
âœ… Usando modelo: llama-3.1-70b-versatile
```

## ğŸ› SoluciÃ³n de Problemas

### Todos los modelos fallan

Si ves:
```
âŒ NingÃºn modelo de Groq disponible, usando fallback
```

**Causas posibles**:
1. Sin conexiÃ³n a internet
2. Keys de Groq invÃ¡lidas
3. Groq estÃ¡ caÃ­do (raro)

**SoluciÃ³n**:
```cmd
# Probar keys
.\test-groq-keys.bat

# Detectar modelos
detect-models.bat

# Verificar conexiÃ³n
curl https://api.groq.com
```

### Quiero usar un modelo especÃ­fico

Edita `src/ai/ai_assistant.py` y pon tu modelo preferido primero en la lista.

## ğŸ“ Logs Ãštiles

El sistema genera logs informativos:

```
âš ï¸ Modelo 'mixtral-8x7b-32768' no disponible, probando siguiente...
âœ… Usando modelo: llama-3.1-70b-versatile
ğŸ”„ Rotando API key: ...KmiTZ â†’ ...VJp2 (RazÃ³n: rate_limit)
```

---

**Â¡El sistema ahora es completamente automÃ¡tico y resiliente!** ğŸ‰

No necesitas preocuparte por modelos descontinuados, el sistema se adapta automÃ¡ticamente.
